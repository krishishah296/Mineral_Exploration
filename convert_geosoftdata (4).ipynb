{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db35f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geosoft.gxpy.gdb as gxgdb\n",
    "import geosoft.gxpy.gx as gx\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "import win32com.client\n",
    "from tqdm import tqdm\n",
    "import geosoft.gxapi as gxapi\n",
    "import datetime\n",
    "import json # <--- NEW: Import for saving/loading JSON\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Constants ---\n",
    "COLOR_RED = '\\033[91m'\n",
    "COLOR_BLUE = '\\033[94m'\n",
    "COLOR_GREEN = '\\033[92m'\n",
    "COLOR_YELLOW = '\\033[93m'\n",
    "RESET = '\\033[0m'\n",
    "UTM_LOOKUP_CACHE_FILE = \"utm_zone_lookup_cache.json\" # <--- NEW: Define cache file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052bd5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T19:45:59.396546Z",
     "start_time": "2025-05-09T19:42:43.472868Z"
    }
   },
   "outputs": [],
   "source": [
    "failed_files_path_only = []\n",
    "\n",
    "# --- Helper Functions (Your existing functions) ---\n",
    "\n",
    "class HeaderDetectionError(Exception):\n",
    "    def __init__(self, message, filepath=None):\n",
    "        super().__init__(message)\n",
    "        self.filepath = filepath\n",
    "\n",
    "def is_number(s):\n",
    "    '''\n",
    "    detect if s contains a number.\n",
    "    s (string): a value to check.\n",
    "    '''\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_wildcard(s):\n",
    "    '''\n",
    "    detect if s contains a wildcard.\n",
    "    s (string): a value to check.\n",
    "    '''\n",
    "    return s in ('', '*', None, 'NA', 'null', 'n/a')\n",
    "\n",
    "def has_same_format(list1, list2):\n",
    "    '''\n",
    "    determining if two strings have the same format across all items.\n",
    "\n",
    "    list1 (list): the first list to compare\n",
    "    list2 (list): the second list to compare\n",
    "    '''\n",
    "    for a, b in zip(list1, list2):\n",
    "        if is_wildcard(a) or is_wildcard(b):\n",
    "            continue\n",
    "        if is_number(a) and is_number(b):\n",
    "            continue\n",
    "        if not is_number(a) and not is_number(b):\n",
    "            continue\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def return_number(list):\n",
    "    '''\n",
    "    finds and returns a number coming after 'UTM' regardless of casing\n",
    "    list (list): a list containing at least one string\n",
    "    '''\n",
    "    match = re.search(r'(?:utm|zone)[^a-zA-Z\\d]*?(\\d+)', list[0], re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def looks_like_header(row):\n",
    "    # Check if majority of the elements are non-numeric strings\n",
    "    non_numeric_count = 0\n",
    "    for elem in row:\n",
    "        if elem is None or (isinstance(elem, str) and elem.strip() == ''):\n",
    "            non_numeric_count += 1\n",
    "            continue\n",
    "        try:\n",
    "            float(elem)\n",
    "        except (ValueError, TypeError):\n",
    "            non_numeric_count += 1\n",
    "    return non_numeric_count > len(row) / 2\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    # This function is the primary cause of slowness when repeatedly called.\n",
    "    # It requires Microsoft Word to be installed.\n",
    "    word = win32com.client.Dispatch(\"Word.Application\")\n",
    "    word.Visible = False\n",
    "    doc = word.Documents.Open(doc_path)\n",
    "    file_text = doc.Content.Text\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "    return file_text\n",
    "\n",
    "def extract_utm_and_zone(file_text):\n",
    "    utm = None\n",
    "    zone = None\n",
    "    lines = file_text.lower().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line_lower = line.lower()\n",
    "\n",
    "        # First try to extract NAD-83 + Zone like '18N'\n",
    "        if \"utm\" in line_lower and \"nad\" in line_lower:\n",
    "            utm_match = re.search(r\"(nad-?\\d+)\", line, re.IGNORECASE)\n",
    "            zone_match = re.search(r\"zone\\s*(\\d+[a-z]?)\", line, re.IGNORECASE)\n",
    "\n",
    "            if utm_match:\n",
    "                utm = utm_match.group(1).upper()\n",
    "            if zone_match:\n",
    "                zone = zone_match.group(1).upper()\n",
    "            if utm and zone:\n",
    "                break\n",
    "\n",
    "        # If not found, fallback to older generic pattern\n",
    "        if utm is None or zone is None:\n",
    "            utm_match = re.search(r\"utm\\s+([a-z0-9]+)\", line_lower)\n",
    "            zone_match = re.search(r\"zone\\s*\\(?\\s*(\\d+)\", line_lower)\n",
    "\n",
    "            if utm_match and utm is None:\n",
    "                utm = utm_match.group(1).upper()\n",
    "            if zone_match and zone is None:\n",
    "                zone = zone_match.group(1)\n",
    "            if utm and zone:\n",
    "                break\n",
    "            \n",
    "    return utm, zone\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Check all .doc files in the folder\n",
    "    doc_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.doc')]\n",
    "    \n",
    "    if not doc_files:\n",
    "        return None\n",
    "\n",
    "    for doc_file in doc_files:\n",
    "        doc_path = os.path.join(folder_path, doc_file)\n",
    "        try:\n",
    "            text = extract_text_from_doc(doc_path)\n",
    "            utm, zone = extract_utm_and_zone(text)\n",
    "            if utm and zone:\n",
    "                return (utm, zone)  # Return the first found pair\n",
    "        except Exception as e:\n",
    "            print(f\"[{os.path.basename(folder_path)}] Error reading {doc_file}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_all_folders_recursively(root_dir, mode=\"create\"):\n",
    "    results = {}\n",
    "    action = \"Creating\" if mode == \"create\" else \"Updating\"\n",
    "    \n",
    "    print(f\"Scanning for .doc/.txt files in: {root_dir} (This might take a while if not cached)...\")\n",
    "    # Using os.walk for tqdm to show progress for folder processing\n",
    "    all_folders = [folder_path for folder_path, _, _ in os.walk(root_dir)]\n",
    "    \n",
    "    for folder_path in tqdm(all_folders, desc=f\"{COLOR_YELLOW}{action} Dictionary of UTM/Zone{RESET}\", colour='yellow'):\n",
    "        res = process_folder(folder_path)\n",
    "        if res:\n",
    "            # Store with relative path or full path as key to avoid name conflicts\n",
    "            # Using full path as key to ensure uniqueness\n",
    "            results[folder_path] = res\n",
    "    return results\n",
    "\n",
    "# --- Cache Management Functions ---\n",
    "def load_utm_lookup_cache(cache_file_path):\n",
    "    if os.path.exists(cache_file_path):\n",
    "        # print(f\"{COLOR_BLUE}Loading UTM lookup from cache: {cache_file_path}\")\n",
    "        try:\n",
    "            with open(cache_file_path, 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                return json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"{COLOR_RED}Error decoding cache file '{cache_file_path}': {e}. Recreating cache.\")\n",
    "            os.remove(cache_file_path) # Delete corrupt cache\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"{COLOR_RED}Error loading cache '{cache_file_path}', recreating: {e}\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_utm_lookup_cache(utm_data, cache_file_path):\n",
    "    print(f\"{COLOR_BLUE}Saving UTM lookup to cache: {cache_file_path}\")\n",
    "    try:\n",
    "        with open(cache_file_path, 'w', encoding='utf-8', errors=\"ignore\") as f:\n",
    "            json.dump(utm_data, f, indent=4)\n",
    "        print(f\"{COLOR_BLUE}Cache saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{COLOR_RED}Error saving cache to {cache_file_path}: {e}\")\n",
    "        \n",
    "def update_utm_lookup_cache(root_directory, cache_file_path):\n",
    "    \"\"\"\n",
    "    Updates (or creates) the UTM cache by re-scanning all folders and adding any new UTM data.\n",
    "    Avoids duplicate keys by comparing normalized full paths.\n",
    "    \"\"\"\n",
    "    # Load existing cache\n",
    "    utm_cache = load_utm_lookup_cache(cache_file_path)\n",
    "    normalized_cache = {os.path.normpath(k).lower(): v for k, v in utm_cache.items()}\n",
    "    updated = False\n",
    "\n",
    "    print(f\"{COLOR_BLUE}Scanning folders to extract UTM info... (this may take a while)\")\n",
    "\n",
    "    # Re-scan all folders for UTM info\n",
    "    new_data = process_all_folders_recursively(root_directory, mode=\"update\")\n",
    "\n",
    "    new_entries = 0\n",
    "    for folder, utm_info in new_data.items():\n",
    "        norm_folder = os.path.normpath(folder).lower()\n",
    "        if norm_folder not in normalized_cache:\n",
    "            utm_cache[folder] = utm_info\n",
    "            normalized_cache[norm_folder] = utm_info\n",
    "            print(f\"{COLOR_GREEN}Added new UTM info for folder: {folder}\")\n",
    "            updated = True\n",
    "            new_entries += 1\n",
    "\n",
    "    if updated:\n",
    "        save_utm_lookup_cache(utm_cache, cache_file_path)\n",
    "        print(f\"{COLOR_GREEN}✓ Cache updated with {new_entries} new entr{'y' if new_entries == 1 else 'ies'}.\")\n",
    "    else:\n",
    "        print(f\"{COLOR_YELLOW}No new UTM data found. Cache is already up to date.\")\n",
    "\n",
    "    return utm_cache\n",
    "\n",
    "# Cleaning up the empty folders in putout directory after all CSV files were created\n",
    "def delete_empty_folders(output_directory):\n",
    "\n",
    "    # Variable to track if any folders were deleted\n",
    "    # This is to ensure we keep checking until no more empty folders are found\n",
    "    not_done = True\n",
    "\n",
    "    while not_done:\n",
    "        # Reset the flag for each iteration\n",
    "        not_done = False\n",
    "        # Walk the directory tree bottom-up\n",
    "        for dirpath, dirnames, filenames in os.walk(output_directory, topdown=False):\n",
    "            if not dirnames and not filenames:  # no subfolders and no files\n",
    "                # print(f\"Deleting empty folder: {dirpath}\")\n",
    "                os.rmdir(dirpath)\n",
    "                # Set the flag to True to indicate a folder was deleted and the loop should run again\n",
    "                not_done = True\n",
    "    \n",
    "    print(f\"{COLOR_GREEN}Output Directory finished cleaning.\")\n",
    "\n",
    "# --- .doc and .txt parsing functions ---\n",
    "def parse_txt_header_1(txt_file):\n",
    "    short_headers = []\n",
    "\n",
    "    with open(txt_file, 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    start_index = next((i for i, line in enumerate(lines) if \"LENGTH\" in line and \"MULTIPLIER\" in line), None)\n",
    "    if start_index is None:\n",
    "        raise ValueError(\"Header section not found\")\n",
    "\n",
    "    for line in lines[start_index+2:]:\n",
    "        line = line.strip()\n",
    "        if not re.match(r\"^\\w+\", line):\n",
    "            break\n",
    "\n",
    "        match = re.match(r\"(\\w+)\\s*:?\\s+(\\d+)\\s+(\\S+)\\s+(.*)\", line)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        field, length, multiplier, comment = match.groups()\n",
    "\n",
    "        if field.upper() == \"BLANK\":\n",
    "            continue\n",
    "\n",
    "        comment = comment.lower()\n",
    "        header = \"\"\n",
    "\n",
    "        if field.upper() == \"EM\":\n",
    "            freq_match = re.search(r'(\\d+)', comment)\n",
    "            freq = freq_match.group(1) if freq_match else \"\"\n",
    "\n",
    "            if \"coaxial\" in comment:\n",
    "                header = f\"Em{freq}coax\"\n",
    "            elif \"coplanar\" in comment:\n",
    "                header = f\"Em{freq}copl\"\n",
    "            else:\n",
    "                header = f\"Em{freq}\"\n",
    "\n",
    "            if \"inphase\" in comment:\n",
    "                header += \"p\"\n",
    "            elif \"quadrature\" in comment:\n",
    "                header += \"q\"\n",
    "\n",
    "        elif field.upper() == \"RESISTIVITY\":\n",
    "            freq_match = re.search(r'(\\d+)', comment)\n",
    "            freq = freq_match.group(1) if freq_match else \"\"\n",
    "            header = f\"Res{freq}\"\n",
    "\n",
    "        elif field.upper() == \"POSITIONING\":\n",
    "            freq_match = re.search(r'([a-z0-9]+)', comment, re.IGNORECASE)\n",
    "            freq = freq_match.group(1) if freq_match else \"\"\n",
    "\n",
    "            comment_lower = comment.lower()\n",
    "            if \"easting\" in comment_lower:\n",
    "                header = \"X\"\n",
    "            elif \"northing\" in comment_lower:\n",
    "                header = \"Y\"\n",
    "\n",
    "        else:\n",
    "            known_map = {\n",
    "                \"LINE\": \"Line\",\n",
    "                \"FLIGHT\": \"Flt\",\n",
    "                \"DATE\": \"Date\",\n",
    "                \"TIME\": \"Time\",\n",
    "                \"MANUAL\": \"Fid\",\n",
    "                \"ALTIMETER\": \"Alti\",\n",
    "                \"MAGNETICS\": \"Magc\",\n",
    "                \"WPM\": \"Pm\"\n",
    "            }\n",
    "            header = known_map.get(field.upper(), field.capitalize())\n",
    "\n",
    "        short_headers.append(header)\n",
    "    return short_headers\n",
    "\n",
    "def parse_txt_header_2(file_path):\n",
    "    keyword_map = {\n",
    "        'x': 'X', 'y': 'Y', 'fiducial': 'Fid', 'flight number': 'Vol',\n",
    "        'date': 'Date', 'time': 'Time', '60 hz detect.': 'Hz60',\n",
    "        'barometric altimeter': 'Altibaro', 'radar altimeter': 'Altiradar',\n",
    "        'mag corrected': 'Magc', 'mag original': 'Mago',\n",
    "        'vlflt ( corrected )': 'VLFLT corrected', 'vlflq ( corrected )': 'VLFLQ corrected',\n",
    "        'vlfot ( corrected )': 'VLFOT corrected', 'vlfoq ( corrected )': 'VLFOQ corrected',\n",
    "        'vlflt ( original )': 'VLFLT original', 'vlflq ( original )': 'VLFLQ original',\n",
    "        'vlfot ( original )': 'VLFOT original', 'vlfoq ( original )': 'VLFOQ original',\n",
    "        'zgps': 'ZGPS', 'k corrected (potassium)': 'K corrected',\n",
    "        'th corrected (thorium)': 'Th corrected', 'u corrected (uranium)': 'U corrected',\n",
    "        'tc corrected (total counts)': 'Total Count corrected',\n",
    "        'tcexp corrected (total count exposure)': 'Total count Exposure corrected',\n",
    "        'k original (potassium)': 'K original', 'th original (thorium)': 'Th original',\n",
    "        'u original (uranium)': 'U original', 'tc original (total counts)': 'Total Count original',\n",
    "        'longitude': 'Longitude', 'latitude': 'Latitude',\n",
    "        'mag igrf': 'MAG igrf', 'zpnav': 'Zpnav', 'mag' : 'MAG',\n",
    "        'vlf line quadrature': 'VLFLQ', 'vlf line total': 'VLFLT',\n",
    "        'vlf ortho quadrature': 'VLFOQ', 'vlf ortho total': 'VLFOT',\n",
    "    }\n",
    "\n",
    "    em_pattern = re.compile(\n",
    "        r'F\\d+\\s*-\\s*(\\d+)\\s*-\\s*(corrected|original)\\s+(coaxial|coplanar)\\s+(inphase|quadrature)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    resistivity_pattern = re.compile(r'resistivity\\s*\\(\\s*[-]?(\\d+)\\s*Hz-?\\s*\\)', re.IGNORECASE)\n",
    "\n",
    "    headers = []\n",
    "\n",
    "    with open(file_path, 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not re.match(r'^\\d+\\s', line):\n",
    "                continue\n",
    "\n",
    "            parts = line.split(maxsplit=2)\n",
    "            if len(parts) == 2:\n",
    "                field_desc = parts[1].strip().lower()\n",
    "            elif len(parts) == 3:\n",
    "                field_desc = parts[2].strip().lower()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            header = None\n",
    "            em_match = em_pattern.search(field_desc)\n",
    "            if em_match:\n",
    "                freq = em_match.group(1)\n",
    "                geom = 'coax' if 'coax' in em_match.group(3).lower() else 'copl' # Corrected group index\n",
    "                phase = 'p' if 'inphase' in em_match.group(4).lower() else 'q' # Corrected group index\n",
    "                header = f\"Em{freq}{geom}{phase}\"\n",
    "\n",
    "            elif \"resistivity\" in field_desc:\n",
    "                res_match = resistivity_pattern.search(field_desc)\n",
    "                if res_match:\n",
    "                    res_freq = res_match.group(1)\n",
    "                    header = f\"Res{res_freq}\"\n",
    "\n",
    "            else:\n",
    "                for key, val in keyword_map.items():\n",
    "                    if field_desc == key:\n",
    "                        header = val\n",
    "                        break\n",
    "                    if field_desc.startswith(key + ' '):\n",
    "                        header = val\n",
    "                        break\n",
    "\n",
    "            if not header:\n",
    "                header = \"Unknown\"\n",
    "\n",
    "            headers.append(header)\n",
    "    return headers\n",
    "\n",
    "def parse_doc_header_1(file_path):\n",
    "    headers = []\n",
    "    in_format_section = False\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if \"SURVEY DATA FORMAT\" in line.upper():\n",
    "                in_format_section = True\n",
    "                continue\n",
    "            if in_format_section and (line.startswith(\"PARAMETER DESCRIPTIONS\") or line.startswith(\"*****\")):\n",
    "                break\n",
    "\n",
    "            if in_format_section:\n",
    "                match = re.match(r'^\\s*\\d+\\s+(\\S+)', line)\n",
    "                if match:\n",
    "                    param = match.group(1)\n",
    "                    headers.append(param)\n",
    "    return headers\n",
    "\n",
    "def header_detection(xyz_path, missing_counter, utm_lookup_dict): # <--- NEW: Pass utm_lookup_dict\n",
    "    xyz_directory = os.path.dirname(xyz_path)\n",
    "    txt_doc_file_path = None\n",
    "    \n",
    "    # Check utm_lookup_dict first (efficient for .doc files)\n",
    "    # This is a bit tricky since header_detection might be called for individual XYZ files\n",
    "    # whose parent folder might already be in utm_lookup_dict.\n",
    "    # For XYZ files, header info is often in a .txt or .doc *within the same folder*,\n",
    "    # not necessarily just the parent folder name.\n",
    "    \n",
    "    # Find matching .txt or .doc file in the same directory as the XYZ file\n",
    "    for filename in os.listdir(xyz_directory):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        # Check if the text/doc file's name contains the XYZ file's name\n",
    "        # or if it's the folder's name (common for summary docs)\n",
    "        if (os.path.splitext(os.path.basename(xyz_path))[0].lower() in name.lower() or\n",
    "            os.path.basename(xyz_directory).lower() in name.lower()) and ext.lower() in ['.txt', '.doc']:\n",
    "            txt_doc_file_path = os.path.join(xyz_directory, filename)\n",
    "            break\n",
    "            \n",
    "    if txt_doc_file_path is None:\n",
    "        missing_counter[0] += 1\n",
    "        failed_files_path_only.append(xyz_path)\n",
    "        raise HeaderDetectionError(f\"No matching TXT/DOC file found for: {xyz_path}\", xyz_path)\n",
    "\n",
    "    print(f\"Attempting to parse header from: {txt_doc_file_path}\")\n",
    "\n",
    "    ext = os.path.splitext(txt_doc_file_path)[1].lower()\n",
    "\n",
    "    if ext == '.txt':\n",
    "        try:\n",
    "            headers = parse_txt_header_1(txt_doc_file_path)\n",
    "            if headers: return headers\n",
    "        except Exception as e:\n",
    "            pass # print(f\"Error in parse_txt_header_1: {e}\")\n",
    "\n",
    "        try:\n",
    "            headers = parse_txt_header_2(txt_doc_file_path)\n",
    "            if headers: return headers\n",
    "        except Exception as e:\n",
    "            pass # print(f\"Error in parse_txt_header_2: {e}\")\n",
    "\n",
    "    elif ext == '.doc':\n",
    "        # Here we can directly use the cached data for the folder if available\n",
    "        # But this function is about header parsing, not UTM/Zone lookup for entire folder.\n",
    "        # So we'll stick to extracting from the doc here, but only if it's the *specific* doc\n",
    "        # that serves as a header file for the XYZ.\n",
    "        try:\n",
    "            headers = parse_doc_header_1(txt_doc_file_path)\n",
    "            if headers: return headers\n",
    "        except Exception as e:\n",
    "            pass # print(f\"Error in parse_doc_header_1: {e}\")\n",
    "\n",
    "    print(f\"{COLOR_RED}No headers found by either parser for {os.path.basename(xyz_path)} in its associated text/doc file.\")\n",
    "    return None\n",
    "\n",
    "def convert_gdb(gdb_file_path, csv_directory):\n",
    "    \"\"\"\n",
    "    This function converts a Geosoft database (.gdb) to a CSV file.\n",
    "    :param gdb_file_path: The path to the GDB file.\n",
    "    :param csv_directory: The directory to save the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(csv_directory, exist_ok=True)\n",
    "\n",
    "        with gxgdb.Geosoft_gdb.open(gdb_file_path) as gdb:\n",
    "            lines = gdb.list_lines()\n",
    "            channels = gdb.list_channels()\n",
    "            gdb_data = []\n",
    "\n",
    "            has_time_utc = \"time_utc\" in channels\n",
    "\n",
    "            for line in lines:\n",
    "                channel_data = {}\n",
    "                max_rows = 0\n",
    "\n",
    "                for channel in channels:\n",
    "                    try:\n",
    "                        values, _ = gdb.read_channel(line, channel)\n",
    "                        values = list(values)\n",
    "                        channel_data[channel] = values\n",
    "                        max_rows = max(max_rows, len(values))\n",
    "                    except Exception as e:\n",
    "                        print(f\"{COLOR_RED}Skipping column: {channel} in line: {line} because: {e}\")\n",
    "                        channel_data[channel] = []\n",
    "\n",
    "                for channel in channels:\n",
    "                    if len(channel_data[channel]) < max_rows:\n",
    "                        channel_data[channel] += [None] * (max_rows - len(channel_data[channel]))\n",
    "\n",
    "                for i in range(max_rows):\n",
    "                    row = {}\n",
    "                    if has_time_utc:\n",
    "                        row[\"L1010\"] = channel_data[\"time_utc\"][i]\n",
    "                    for channel in channels:\n",
    "                        if channel != \"time_utc\":\n",
    "                            row[channel] = channel_data[channel][i]\n",
    "                    gdb_data.append(row)\n",
    "                    \n",
    "            # Load UTM lookup cache and normalize keys\n",
    "            with open(UTM_LOOKUP_CACHE_FILE, 'r') as f:\n",
    "                raw_cache = json.load(f)\n",
    "                utm_lookup_dict = {os.path.normpath(k).lower(): v for k, v in raw_cache.items()}\n",
    "\n",
    "            # Normalize the folder path of the input GDB file\n",
    "            xyz_folder_path = os.path.normpath(os.path.dirname(gdb_file_path)).lower()\n",
    "\n",
    "            # Try to find UTM and Zone info for this folder path\n",
    "            utm_zone = None\n",
    "            if xyz_folder_path in utm_lookup_dict:\n",
    "                utm_zone = utm_lookup_dict[xyz_folder_path]\n",
    "            else:\n",
    "                # Fallback: check if any cached folder path is a prefix of this folder path\n",
    "                for cached_path, value in utm_lookup_dict.items():\n",
    "                    if xyz_folder_path.startswith(cached_path):\n",
    "                        if isinstance(value, list) and len(value) == 2:\n",
    "                            utm_zone = value\n",
    "                            break\n",
    "\n",
    "            # If not found, fill with string 'NaN'\n",
    "            if utm_zone is None:\n",
    "                utm_zone = ['NaN', 'NaN']\n",
    "\n",
    "            # Prepare the header row\n",
    "            header = []\n",
    "\n",
    "            # If gdb_data is a list of dicts, convert first dict keys to header (adjust if needed)\n",
    "            if isinstance(gdb_data[0], dict):\n",
    "                header = list(gdb_data[0].keys())\n",
    "            else:\n",
    "                # If gdb_data is a list of lists, you may already have header elsewhere or need to define it\n",
    "                # For this example, just create an empty header to be extended below\n",
    "                header = []\n",
    "\n",
    "            # Ensure 'L1010' is added if 'time_utc' exists\n",
    "            if has_time_utc and 'L1010' not in header:\n",
    "                header.append(\"L1010\")\n",
    "\n",
    "            # Append all channels except 'time_utc' (avoid duplicates)\n",
    "            for ch in channels:\n",
    "                if ch != \"time_utc\" and ch not in header:\n",
    "                    header.append(ch)\n",
    "\n",
    "            # Append 'UTM' and 'Zone' to header if not already there\n",
    "            if 'UTM' not in header:\n",
    "                header.append('UTM')\n",
    "            if 'Zone' not in header:\n",
    "                header.append('Zone')\n",
    "\n",
    "            # Append UTM and Zone values to each row in gdb_data\n",
    "            updated_data = []\n",
    "            for row in gdb_data:\n",
    "                if isinstance(row, dict):\n",
    "                    # Add or update UTM and Zone keys\n",
    "                    row['UTM'] = utm_zone[0]\n",
    "                    row['Zone'] = utm_zone[1]\n",
    "                    updated_data.append(row)\n",
    "                else:\n",
    "                    # Assuming row is a list, append utm_zone values\n",
    "                    updated_data.append(row + utm_zone)\n",
    "\n",
    "            gdb_data = updated_data\n",
    "\n",
    "            dataframe = pd.DataFrame(gdb_data)\n",
    "\n",
    "            if has_time_utc:\n",
    "                expected_order = [\"L1010\"] + [col for col in dataframe.columns if col != \"L1010\"]\n",
    "                dataframe = dataframe[expected_order]\n",
    "\n",
    "            csv_file = os.path.join(csv_directory, f\"{os.path.splitext(os.path.basename(gdb_file_path))[0]}_gdb.csv\")\n",
    "            dataframe.to_csv(csv_file, index=False, sep=\",\", encoding=\"utf-8\", escapechar=None, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "            print(f\"{COLOR_BLUE}Successfully converted {gdb_file_path} to {csv_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{COLOR_RED}ERROR in processing {gdb_file_path} because: {e}\")\n",
    "        return e\n",
    "\n",
    "def convert_xyz_to_csv(xyz_file_path, csv_directory, missing_counter, mismatching_files_counter, utm_lookup_dict): # <--- NEW: Pass utm_lookup_dict\n",
    "    \"\"\"\n",
    "    Converts a Geosoft XYZ file to a CSV file without extra quotes around values.\n",
    "    :param xyz_file_path: The path to the XYZ file.\n",
    "    :param csv_directory: The directory to save the CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(csv_directory, exist_ok=True)\n",
    "    csv_file_name = os.path.splitext(os.path.basename(xyz_file_path))[0] + \".csv\"\n",
    "    csv_file_path = os.path.join(csv_directory, csv_file_name)\n",
    "    print(csv_file_path)\n",
    "\n",
    "    with open(xyz_file_path, encoding=\"utf-8\", errors=\"ignore\") as xyz_file:\n",
    "        lines = xyz_file.readlines()\n",
    "\n",
    "    print(f\"Loaded: {xyz_file_path}\")\n",
    "\n",
    "    lengths = [len(s.strip().split()) for s in lines]\n",
    "    counter = Counter(lengths)\n",
    "    most_common_length = counter.most_common(1)[0][0]\n",
    "    normalized_lines = [s.strip().split() for s in lines if s.strip()]\n",
    "\n",
    "    header = None # Initialize header\n",
    "    header_line = \"\" # Initialize header_line\n",
    "    header_source_line_clean = None # Initialize for the try/except block\n",
    "\n",
    "    try:\n",
    "        header_search = [s for s in lines if len(s.strip().split()) > most_common_length and '---' not in s and '===' not in s]\n",
    "        if header_search:\n",
    "            header_source_line = header_search[-1]\n",
    "            if ',' in header_source_line:\n",
    "                if header_source_line.startswith('/'):\n",
    "                    header_source_line = header_source_line[1:] # Removes the first /\n",
    "                header_source_line_clean = header_source_line.strip() # Removes white spaces\n",
    "                header_line = [item.strip() for item in header_source_line_clean.split(',')]\n",
    "            else:\n",
    "                header_line = header_search[-1].strip().split()\n",
    "                header_source_line = header_search[-1]\n",
    "                header_source_line_clean = header_source_line.strip()\n",
    "    except Exception: # Catch any error during header_search\n",
    "        pass # Will proceed to header_detection if header_line is still None\n",
    "\n",
    "    data_setup = [s for s in lines if len(s.strip().split()) == most_common_length and '---' not in s and '===' not in s]\n",
    "\n",
    "    if not data_setup:\n",
    "        print(f\"{COLOR_RED}No data lines found with consistent length in {xyz_file_path}.\")\n",
    "        return\n",
    "\n",
    "    last_row = data_setup[-1].strip().split()\n",
    "    first_row = data_setup[0].strip().split()\n",
    "\n",
    "    if header is None: # If header not determined from initial search\n",
    "        if header_line: # If we found a candidate header_line from the initial search\n",
    "            header_vs_data = len(header_line) - len(last_row)\n",
    "            if header_vs_data >= 0: # Ensure index is not negative\n",
    "                header = header_line[header_vs_data:]\n",
    "            else: # If header_line is shorter than data, it's likely not the right header\n",
    "                header = header_detection(xyz_file_path, missing_counter, utm_lookup_dict) # Fallback to external detection\n",
    "        else: # No header found from initial search, go straight to external detection\n",
    "            header = header_detection(xyz_file_path, missing_counter, utm_lookup_dict)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Re-evaluate data processing logic (simplified based on typical XYZ structures)\n",
    "    # The original logic for has_same_format(last_row, first_row) and finding the start of data\n",
    "    # seems overly complex and potentially error-prone for typical XYZ files.\n",
    "    # Usually, if there's a header, data starts immediately after it.\n",
    "    # The most common length lines are usually the data.\n",
    "\n",
    "    # Simpler approach:\n",
    "    # If a valid header was found, data starts right after the header line.\n",
    "    # If no header was found internally, assume all most_common_length lines are data.\n",
    "\n",
    "    if looks_like_header(header):\n",
    "        header = header\n",
    "    else:\n",
    "        header = None\n",
    "    \n",
    "    # Single format case: all data lines same format\n",
    "    data = [s.strip().split() for s in data_setup if s.strip()]\n",
    "    # Only assign header if not already set or if header length mismatches data length\n",
    "    if (not header) or (len(header) != len(data[0])):\n",
    "        if header_line and len(header_line) == len(data[0]):\n",
    "            header = header_line\n",
    "        else:\n",
    "            try:\n",
    "                # Try to detect header externally\n",
    "                header_candidate = header_detection(xyz_file_path, missing_counter, utm_lookup_dict)\n",
    "            except HeaderDetectionError as e:\n",
    "                # Re-raise so it exits the function\n",
    "                raise e\n",
    "\n",
    "            # If header was found but doesn't match data, raise error \n",
    "            if header_candidate:\n",
    "                if len(header_candidate) != len(data[0]):\n",
    "                    mismatching_files_counter [0] += 1\n",
    "                    raise HeaderDetectionError(\n",
    "                        f\"Header found from external source (.txt or .doc file), but its length ({len(header_candidate)}) \"\n",
    "                        f\"does not match data length ({len(data[0])}) in file: {xyz_file_path}\",\n",
    "                        xyz_file_path\n",
    "                    )\n",
    "                else:\n",
    "                    header = header_candidate\n",
    "            \n",
    "    # Find lines that weren't used in header or data\n",
    "    used_lines_set = set(tuple(row) for row in (data + ([header] if header else [])))\n",
    "    unused_lines = [original_line for original_line, split_line in zip(lines, normalized_lines)\n",
    "                    if tuple(split_line) not in used_lines_set]\n",
    "\n",
    "    # Ensure header is a list and not a tuple (handle (None, None) from header_detection)\n",
    "    if isinstance(header, tuple):\n",
    "        # If header is (None, None), treat as missing header\n",
    "        if all(h is None for h in header):\n",
    "            if data and len(data) > 0:\n",
    "                header = [f\"Col{i+1}\" for i in range(len(data[0]))]\n",
    "            else:\n",
    "                header = [\"Col1\", \"Col2\", \"Col3\"]\n",
    "        else:\n",
    "            header = list(header)\n",
    "\n",
    "    # Load the cache file and normalize paths\n",
    "    with open(UTM_LOOKUP_CACHE_FILE, 'r') as f:\n",
    "        raw_cache = json.load(f)\n",
    "        utm_lookup_dict = {os.path.normpath(k).lower(): v for k, v in raw_cache.items()}\n",
    "\n",
    "    # Get the folder path of the XYZ file\n",
    "    xyz_folder_path = os.path.normpath(os.path.dirname(xyz_file_path)).lower()\n",
    "\n",
    "    # Try to find UTM and Zone from dictionary using file path substring\n",
    "    utm_zone = None\n",
    "\n",
    "    # Prioritize lookup by exact folder path\n",
    "    if xyz_folder_path in utm_lookup_dict:\n",
    "        utm_zone = utm_lookup_dict[xyz_folder_path]\n",
    "    else:\n",
    "        # Fallback to looking by folder name\n",
    "        for folder_path_in_cache, value in utm_lookup_dict.items():\n",
    "            if xyz_folder_path.startswith(folder_path_in_cache):\n",
    "                if isinstance(value, list) and len(value) == 2:\n",
    "                    utm_zone = value\n",
    "                    break\n",
    "\n",
    "    # Add headers for UTM and Zone\n",
    "    if header and 'UTM' not in header and 'Zone' not in header:\n",
    "        header.extend(['UTM', 'Zone'])\n",
    "    elif not header:\n",
    "        header = ['Col1', 'Col2', 'Col3', 'UTM', 'Zone']\n",
    "\n",
    "    # Append to each row of data\n",
    "    if utm_zone and isinstance(utm_zone, list) and len(utm_zone) == 2:\n",
    "        utm_val, zone_val = utm_zone\n",
    "        data = [row + [utm_val, zone_val] for row in data]\n",
    "    else:\n",
    "        data = [row + ['', ''] for row in data]\n",
    "\n",
    "\n",
    "    # --- Handle missing values: replace empty strings or None with 'NaN' ---\n",
    "    missing_rows = 0\n",
    "    def replace_missing(row):\n",
    "        nonlocal missing_rows\n",
    "        cleaned = []\n",
    "        row_missing = False\n",
    "        for val in row:\n",
    "            if val is None or (isinstance(val, str) and val.strip() == ''):\n",
    "                cleaned.append('NaN')\n",
    "                row_missing = True\n",
    "            else:\n",
    "                cleaned.append(val)\n",
    "        if row_missing:\n",
    "            missing_rows += 1\n",
    "        return cleaned\n",
    "    data = [replace_missing(row) for row in data]\n",
    "    if missing_rows > 0:\n",
    "        print(f\"{COLOR_GREEN}Info: {missing_rows} row(s) in {xyz_file_path} had missing values replaced with 'NaN'.\")\n",
    "\n",
    "    # This removes all of the potentially extra commas that may exist in the header\n",
    "    joined = \",\".join(header)\n",
    "    header = [col.strip() for col in joined.split(\",\") if col.strip()]\n",
    "\n",
    "    print(f\"Saving CSV to directory: {csv_file_path}\")\n",
    "    with open(csv_file_path, 'w', newline='', encoding=\"utf-8\", errors=\"ignore\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',', quoting=csv.QUOTE_NONE, escapechar=\"\\\\\");\n",
    "\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"{COLOR_BLUE}Conversion Complete, file placed in: {csv_file_path}\")\n",
    "    clean_up_csv(csv_file_path)\n",
    "\n",
    "\n",
    "def clean_up_csv(csv_path): # Changed parameter name for clarity\n",
    "    \"\"\"\n",
    "    Cleans up the CSV file by removing escape characters (slashes) that are added during the conversion process.\n",
    "    :param csv_path: The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(csv_path, 'r', newline='', encoding=\"utf-8\", errors=\"ignore\") as csv_file: # Read with newline='' to avoid issues\n",
    "            lines = csv_file.readlines()\n",
    "\n",
    "        cleaned_lines = [line.replace(\"\\\\\", \"\") for line in lines]\n",
    "\n",
    "        with open(csv_path, 'w', newline='', encoding=\"utf-8\", errors=\"ignore\") as csv_file: # Write with newline=''\n",
    "            csv_file.writelines(cleaned_lines)\n",
    "\n",
    "        print(f\"{COLOR_BLUE}CSV File Cleaned up: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{COLOR_RED}Error cleaning up CSV {csv_path}: {e}\")\n",
    "\n",
    "\n",
    "def csv_carry_over(csv_file_path, output_directory):\n",
    "    \"\"\"\n",
    "    Carries over an existing CSV file to the output directory with UTM and Zone addition modification.\n",
    "    :param csv_file_path: The path to the input CSV file.\n",
    "    :param output_directory: The directory to save the converted CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    csv_file_name = os.path.basename(csv_file_path)\n",
    "    output_csv_path = os.path.join(output_directory, csv_file_name)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "        header = df.columns.tolist()  # Convert Index to list\n",
    "        data = df.values.tolist()\n",
    "        \n",
    "        # Load the cache file and normalize paths\n",
    "        with open(UTM_LOOKUP_CACHE_FILE, 'r') as f:\n",
    "            raw_cache = json.load(f)\n",
    "            utm_lookup_dict = {os.path.normpath(k).lower(): v for k, v in raw_cache.items()}\n",
    "\n",
    "        # Get the folder path of the CSV file\n",
    "        csv_folder_path = os.path.normpath(os.path.dirname(csv_file_path)).lower()\n",
    "\n",
    "        # Try to find UTM and Zone from dictionary using file path substring\n",
    "        utm_zone = None\n",
    "\n",
    "        # Prioritize lookup by exact folder path\n",
    "        if csv_folder_path in utm_lookup_dict:\n",
    "            utm_zone = utm_lookup_dict[csv_folder_path]\n",
    "        else:\n",
    "            # Fallback to looking by folder name\n",
    "            for folder_path_in_cache, value in utm_lookup_dict.items():\n",
    "                if csv_folder_path.startswith(folder_path_in_cache):\n",
    "                    if isinstance(value, list) and len(value) == 2:\n",
    "                        utm_zone = value\n",
    "                        break\n",
    "\n",
    "        # Add headers for UTM and Zone\n",
    "        if header and 'UTM' not in header and 'Zone' not in header:\n",
    "            header.extend(['UTM', 'Zone'])\n",
    "        elif not header:\n",
    "            header = ['Col1', 'Col2', 'Col3', 'UTM', 'Zone']\n",
    "\n",
    "        # Append to each row of data\n",
    "        if utm_zone and isinstance(utm_zone, list) and len(utm_zone) == 2:\n",
    "            utm_val, zone_val = utm_zone\n",
    "            data = [row + [utm_val, zone_val] for row in data]\n",
    "        else:\n",
    "            data = [row + ['', ''] for row in data]\n",
    "\n",
    "        print(f\"Saving CSV to directory: {output_csv_path}\")\n",
    "        with open(output_csv_path, 'w', newline='', encoding=\"utf-8\", errors=\"ignore\") as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',', quoting=csv.QUOTE_NONE, escapechar=\"\\\\\");\n",
    "\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(data)\n",
    "\n",
    "        print(f\"{COLOR_BLUE}Carry Over Complete, file placed in: {csv_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{COLOR_RED}Error in carrying over CSV {csv_file_path}: {e}\")\n",
    "\n",
    "\n",
    "def file_finder(root_directory, output_directory, utm_lookup_dict): # <--- NEW: Pass utm_lookup_dict\n",
    "    \"\"\"\n",
    "    This function finds all GDB and XYZ files in the given directory and converts them to CSV files.\n",
    "    :param root_directory: The root directory to search for GDB/XYZ files.\n",
    "    :param output_directory: The base directory to save the CSV files.\n",
    "    :param utm_lookup_dict: Dictionary containing pre-extracted UTM/Zone info.\n",
    "    \"\"\"\n",
    "    gdb_found = 0\n",
    "    xyz_found = 0\n",
    "    csv_found = 0\n",
    "    gdb_converted = 0\n",
    "    gdb_convertion_errors = 0\n",
    "    xyz_converted = 0\n",
    "    csv_carried = 0\n",
    "    csv_carry_error = 0\n",
    "    missing_files_counter = [0]\n",
    "    mismatching_files_counter = [0]\n",
    "    unknown_error_counter = 0\n",
    "    failed_files = []\n",
    "\n",
    "    # Initialize Geosoft context (this only needs to happen once)\n",
    "    # Setup Geosoft context if it doesn't already exist\n",
    "    try:\n",
    "        gx_context = gx.GXpy()  # try to create context\n",
    "    except gxapi.GXAPIError as e:\n",
    "        if \"already been created\" in str(e):\n",
    "            pass\n",
    "\n",
    "    print(f\"\\n{COLOR_BLUE}--- Starting file conversion ---\")\n",
    "    # Recursively loops through all folders and subfolders finding all GDB/XYZ files\n",
    "    # Using tqdm for progress on file iteration\n",
    "    all_folders = [folder for folder, _, _ in os.walk(root_directory)]\n",
    "\n",
    "    for folder in tqdm(all_folders, desc=f\"{COLOR_YELLOW}Processing .gdb and .xyz files{RESET}\", colour='yellow'):\n",
    "        # We need to iterate over files within the current folder for tqdm to count items correctly per folder\n",
    "        for file in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, file)\n",
    "            # Create mirrored output directory path for the current file's folder\n",
    "            rel_path = os.path.relpath(folder, root_directory)\n",
    "            output_dir = os.path.join(output_directory, rel_path)\n",
    "            os.makedirs(output_dir, exist_ok=True) # Ensure output dir exists for current file\n",
    "\n",
    "            if file.lower().endswith(\".gdb\"):\n",
    "                gdb_found += 1\n",
    "                try:\n",
    "                    convert_gdb(file_path, output_dir)\n",
    "                    gdb_converted += 1\n",
    "                except Exception as ex:\n",
    "                    gdb_convertion_errors += 1\n",
    "                    print(f'{COLOR_RED}❌ Fail to extract {file_path} because: {ex}')\n",
    "                    failed_files.append(f'❌ Failed to extract: {file_path} - Reason: {ex}')\n",
    "                    failed_files_path_only.append(file_path)\n",
    "\n",
    "            elif file.lower().endswith(\".xyz\"):\n",
    "                xyz_found += 1\n",
    "                try:\n",
    "                    convert_xyz_to_csv(file_path, output_dir, missing_files_counter, mismatching_files_counter, utm_lookup_dict) # <--- Pass utm_lookup_dict\n",
    "                    xyz_converted += 1\n",
    "                except HeaderDetectionError as hde:\n",
    "                    error_msg = (f\"❌ Header Creation Error: {hde}\")\n",
    "                    print(error_msg)\n",
    "                    failed_files.append(error_msg)\n",
    "                except Exception as ex:\n",
    "                    unknown_error_counter += 1\n",
    "                    print(f'{COLOR_RED}❌ Fail to extract {file_path}') \n",
    "                    failed_files.append(f'{COLOR_RED}❌ Failed to extract: {file_path} - Reason: {ex}')\n",
    "                    failed_files_path_only.append(file_path)\n",
    "\n",
    "            elif file.lower().endswith(('.csv')):\n",
    "                csv_found += 1\n",
    "                try:\n",
    "                    csv_carry_over(file_path, output_dir)\n",
    "                    csv_carried += 1\n",
    "                except Exception as ex:\n",
    "                    csv_carry_error += 1\n",
    "                    print(f'{COLOR_RED}❌ Fail to carry over {file_path} because: {ex}')\n",
    "                    failed_files.append(f'{COLOR_RED}❌ Failed to carry over: {file_path} - Reason: {ex}')\n",
    "                    failed_files_path_only.append(file_path)\n",
    "\n",
    "    # Save failed file paths to a .txt file with error code\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    with open(os.path.join(output_directory, \"failed_files_path.txt\"), \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(\"\\n--- New run: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" ---\\n\")\n",
    "        for path in failed_files:\n",
    "            f.write(path + \"\\n\")\n",
    "\n",
    "    # Save failed file paths to a .txt file with only paths\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    with open(os.path.join(output_directory, \"failed_files_pathsOnly.txt\"), \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for path in failed_files_path_only:\n",
    "            f.write(path + \"\\n\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{COLOR_BLUE}=== Conversion Summary ===\")\n",
    "    print(f\"GDB Files:\")\n",
    "    print(f\" - Files Found: {gdb_found}\")\n",
    "    print(f\" - Files Converted: {gdb_converted}\")\n",
    "    print(f\" - Files Failed to Convert: {gdb_convertion_errors}\")\n",
    "    print(f\"-\" * 20)\n",
    "    print(f\"XYZ Files:\")\n",
    "    print(f\" - Files Found: {xyz_found}\")\n",
    "    print(f\" - Files Converted: {xyz_converted}\")\n",
    "    print(f\" - Missing TXT/DOC files for header detection: {missing_files_counter[0]}\")\n",
    "    print(f\" - Header and Data Mismatching files: {mismatching_files_counter[0]}\")\n",
    "    print(f\" - Unspecified error: {unknown_error_counter}\")\n",
    "    print(f\"-\" * 20)\n",
    "    print(f\"CSV Files:\")\n",
    "    print(f\" - Files Found: {csv_found}\")\n",
    "    print(f\" - Files Carried Over: {csv_carried}\")\n",
    "    print(f\" - Files Failed to Carry Over: {csv_carry_error}\")\n",
    "    print(f\"-\" * 20)\n",
    "    print(f\"{COLOR_BLUE}Conversion process completed.\")\n",
    "\n",
    "\n",
    "### Problematic File Gatherer Code\n",
    "# This code will take a file that contains problematic files, and copy them over to a new folder\n",
    "# so that the user can easily access them and fix them.\n",
    "# It also creates a log of how many files were copied over, and their locations.\n",
    "\n",
    "def copy_problematic_folders(problem_file_path, copied_directory):\n",
    "    # Open the file and read all lines, stripping whitespace, and convert to Path objects\n",
    "    with open(problem_file_path, 'r') as f:\n",
    "        problematic_files = [Path(line.strip()) for line in f if line.strip()]\n",
    "        \n",
    "    # Check that the file has file paths\n",
    "    if not problematic_files:\n",
    "        print(\"No problematic files found.\")\n",
    "        return\n",
    "\n",
    "    # Infer the common project root automatically\n",
    "    project_root = Path(os.path.commonpath([str(f) for f in problematic_files]))\n",
    "    if project_root.is_file():\n",
    "        project_root = project_root.parent\n",
    "\n",
    "    # Ensure destination base exists\n",
    "    copied_directory = Path(copied_directory)\n",
    "    copied_directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize counters\n",
    "    counters = {'.xyz': 0, '.gdb': 0, '.txt': 0, '.doc': 0}\n",
    "    \n",
    "    for file in problematic_files:\n",
    "        # Check to see if the file exist in your directory\n",
    "        if not file.exists():\n",
    "            print(f\"File does not exist: {file}\")\n",
    "            continue\n",
    "        \n",
    "        folder_to_copy = file.parent\n",
    "        \n",
    "        # Prepare list: the actual problem file (.gdb or .xyz from the .txt file) \n",
    "        # and prepares any extra .txt/.doc/.docx files in the same folder\n",
    "        files_to_copy = [file] + [\n",
    "            f for f in folder_to_copy.iterdir()\n",
    "            if f.suffix.lower() in {'.txt', '.doc'} and f.is_file()\n",
    "        ]\n",
    "        \n",
    "        for f in files_to_copy:\n",
    "            # Compute relative path from project root\n",
    "            relative_path = f.relative_to(project_root)\n",
    "            destination_path = copied_directory / relative_path\n",
    "\n",
    "            # Ensure parent exists, then copy the file\n",
    "            destination_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(f, destination_path)\n",
    "            \n",
    "            # Increment counter if extension matches\n",
    "            ext = f.suffix.lower()\n",
    "            if ext in counters:\n",
    "                counters[ext] += 1\n",
    "            \n",
    "            # Uncomment this if you want this to tell you every file that was copied over\n",
    "            # Commented out to reduce the output lines\n",
    "            # print(f\"Copied: {f} -> {destination_path}\")\n",
    "\n",
    "    print(\"Copying complete.\")\n",
    "    print(\"Summary of files copied:\")\n",
    "    \n",
    "    for ext, count in counters.items():\n",
    "        print(f\"  {ext}: {count}\")\n",
    "\n",
    "\n",
    "### Error Checker Code\n",
    "\n",
    "# This is the error finder code that goes through the produced results folder, going through each file and finding the files that contain NaN values for the UTM and Zone values, \n",
    "# missing UTM and Zone labels in headers, or if the second row has NaN values in the UTM and/or Zone, as well as the files that contain unknown header values. \n",
    "# It will find these problamtic files and document them in their respective error TXT files so the user can simply go through that and see the problemtic files. \n",
    "\n",
    "def check_csvs(folder_path):\n",
    "\n",
    "    missing_utm_zone = []\n",
    "    missing_second_row = []\n",
    "    nan_utm_zone = []\n",
    "    reading_error = []\n",
    "\n",
    "    print(f\"Scanning folder: {folder_path}\\n\")\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Read just the first two rows\n",
    "                    df = pd.read_csv(file_path, nrows=2, low_memory=False)\n",
    "\n",
    "                    # Normalize column names\n",
    "                    normalized_columns = [col.strip().lower() for col in df.columns]\n",
    "\n",
    "                    if 'utm' not in normalized_columns or 'zone' not in normalized_columns:\n",
    "                        missing_utm_zone.append(\"[Missing 'utm' or 'zone' in header] {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    utm_col = df.columns[normalized_columns.index('utm')]\n",
    "                    zone_col = df.columns(normalized_columns.index('zone'))\n",
    "\n",
    "                    # Check if second row has NaN in utm or zone\n",
    "                    if len(df) < 2:\n",
    "                        missing_second_row.append(\"[Missing second row] {file_path}\")\n",
    "                    elif str(df.at[1, utm_col]).strip().lower() == \"nan\" or str(df.at[1, zone_col]).strip().lower() == \"nan\":\n",
    "                        nan_utm_zone.append(\"[NaN in UTM or Zone Data] {file_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    reading_error.append(f\"[Error reading file] {file_path} — {str(e)}\")\n",
    "                    \n",
    "    print('Scanning complete.')\n",
    "\n",
    "    error_logs = {\"Missing_UTM_Zone\": missing_utm_zone,\n",
    "                  \"Missing_Second_Row\": missing_second_row,\n",
    "                  \"NaN_UTM_Zone\": nan_utm_zone,\n",
    "                  \"Reading_Error\": reading_error}\n",
    "\n",
    "    for error_type, items in error_logs.items():\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        with open(os.path.join(folder_path, f\"{error_type}.txt\"), \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            f.write(\"\\n--- New run: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" ---\\n\")\n",
    "            for item in items:\n",
    "                f.write(item + \"\\n\")\n",
    "\n",
    "    print(f\"\\n{COLOR_BLUE}Error logs saved in {folder_path} for further review.\")\n",
    "\n",
    "def check_unknown_in_header(folder_path):\n",
    "\n",
    "    unknown_columns = []\n",
    "    reading_error = []\n",
    "\n",
    "    print(f\"Scanning folder: {folder_path}\\n\")\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Read only the header row (no data rows)\n",
    "                    df = pd.read_csv(file_path, nrows=0)\n",
    "\n",
    "                    # Check each column name for \"unknown\" (case-insensitive)\n",
    "                    unknown_columns = [col for col in df.columns if \"unknown\" in col.lower()]\n",
    "                    if unknown_columns:\n",
    "                        unknown_columns.append(\"[Unknown column(s)] {file_path}\")\n",
    "                        for col in unknown_columns:\n",
    "                            unknown_columns.append[f\" - {col}\"]\n",
    "\n",
    "                except Exception as e:\n",
    "                    reading_error.append(\"[Error reading file] {file_path} — {str(e)}\")\n",
    "                    \n",
    "    print('Scanning complete.')\n",
    "\n",
    "    error_logs = {\"Unknown_Columns\": unknown_columns,\n",
    "                  \"Reading_Error\": reading_error}\n",
    "    \n",
    "    for error_type, items in error_logs.items():\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        with open(os.path.join(folder_path, f\"{error_type}.txt\"), \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            f.write(\"\\n--- New run: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" ---\\n\")\n",
    "            for item in items:\n",
    "                f.write(item + \"\\n\")\n",
    "\n",
    "    print(f\"\\n{COLOR_BLUE}Unknown columns and reading errors saved in {folder_path} for further review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Root directory with the .xyz/.gdb files\n",
    "    # IMPORTANT: Set this to the parent directory that contains ALL your DP folders    \n",
    "    root_directory = r'C:\\Users\\ishpr\\Desktop\\Mineral Team\\Preprocessing Team\\Dataset\\DP200002\\Chunk 1'\n",
    "    \n",
    "    # Mimicked directory that will be created to hold the .csv files\n",
    "    # This directory does not need to exist, it will create everything for you in the desired path \n",
    "    output_directory = r'C:\\Users\\ishpr\\Desktop\\Mineral Team\\Preprocessing Team\\Converted_Files'\n",
    "\n",
    "    # This is the .txt file created by the script where the problematic files pathways ONLY are going to be stored \n",
    "    # (Replace with output_directory\\problem_files.txt in the future)\n",
    "    problem_file_path = r'C:\\Users\\ishpr\\Desktop\\Mineral Team\\Preprocessing Team\\Converted_Files\\failed_files_pathsOnly.txt'\n",
    "\n",
    "    # This is the new folder to copy the problematic files into so they can be shared\n",
    "    # (It doesn't need to exist it wll be created when this is run)\n",
    "    copied_directory = r'C:\\Users\\ishpr\\Desktop\\Mineral Team\\Preprocessing Team\\Dataset\\Copied_Problematic_Files'\n",
    "\n",
    "    \n",
    "    \n",
    "    # --- Implement Caching for UTM Lookup ---\n",
    "    utm_lookup = load_utm_lookup_cache(UTM_LOOKUP_CACHE_FILE)\n",
    " \n",
    "    if not utm_lookup:\n",
    "        print(f\"{COLOR_RED}Cache not found or empty. Processing all .doc files for UTM/Zone info. This may take a while...\")\n",
    "        # Make sure root_directory for this step is correct for scanning ALL relevant .doc files\n",
    "        # It needs to be the root of your project structure where folders like DP200601, DP200102 etc. reside.\n",
    "        utm_lookup = process_all_folders_recursively(root_directory, mode=\"create\")\n",
    "        save_utm_lookup_cache(utm_lookup, UTM_LOOKUP_CACHE_FILE)\n",
    "        print(f\"{COLOR_BLUE}UTM lookup processing complete and cached.\")\n",
    "    else:\n",
    "        print(f\"{COLOR_BLUE}Cache file loaded: {UTM_LOOKUP_CACHE_FILE}\")\n",
    "        # Updates the UTM cache (adds only new folders)\n",
    "        utm_lookup = update_utm_lookup_cache(root_directory, UTM_LOOKUP_CACHE_FILE)\n",
    "\n",
    "    # Now proceed with file conversion using the loaded/generated utm_lookup\n",
    "    # Ensure the root_directory for file_finder also covers all files you want to convert\n",
    "    file_finder(root_directory, output_directory, utm_lookup)\n",
    "    \n",
    "    # This will process the output directory and delete any folders that have no files or folders inside it\n",
    "    delete_empty_folders(output_directory)\n",
    "\n",
    "    # P.S. The failed_files_path.txt file will be empty if no files failed to convert.\n",
    "    # P.P.S. The output_directory will be created if it doesn't exist, and the converted files will be placed inside it.\n",
    "\n",
    "\n",
    "\n",
    "    ### Problematic File Gatherer Code Runner\n",
    "    copy_problematic_folders(problem_file_path, copied_directory)\n",
    "\n",
    "\n",
    "\n",
    "    ### Running the error checker to find any problematic files in the output directory\n",
    "    check_csvs(output_directory)\n",
    "    check_unknown_in_header(output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geosoft_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
